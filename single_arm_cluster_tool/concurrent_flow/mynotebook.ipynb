{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-armed cluster tool with concurrent flow wafers \n",
    "\n",
    "* The stage configuration is [1,1,1,1,1,1]\n",
    "    * Type 1 wafers are processed in the order PM1 -> PM2 -> PM3. \n",
    "    * Type 2 wafers are processed in the order PM4 -> PM5 -> PM6. \n",
    "    \n",
    "* Process times are sampled from values between 10-300s. \n",
    "* The baseline robot move sequence uses concurrent backward sequence. \n",
    "* Trained model checkpoint file is loaded from ./saved_models/checkpoint_v4.pt\n",
    "    * Another trained model for (Type 1 flow is PM1->PM2, Type 2 flow is PM3->PM4) is ./saved_models/checkpoint_v1.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from envs.sdcfEnv import sdcfEnv as Env, State\n",
    "from model.model_concat import CONCATNet as CONCATModel\n",
    "from envs.algorithms.cbs import ConcurrentBackwardSequence\n",
    "\n",
    "# Global configurations\n",
    "DEBUG_MODE = True\n",
    "USE_CUDA = not DEBUG_MODE\n",
    "CUDA_DEVICE_NUM = 0\n",
    "SEED = 1000\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Fix random seed for reproducibility.\"\"\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Train a model with a specific number of lot types')\n",
    "    parser.add_argument('--foup_size', type=int, default=100, help='Size of the foup')\n",
    "    parser.add_argument('--group1_stage', type=int, default=1, help='Stages for type 1')\n",
    "    parser.add_argument('--group1_min_prs_time', type=int, default=10, help='Minimum processing time for type 1')\n",
    "    parser.add_argument('--group1_max_prs_time', type=int, default=300, help='Maximum processing time for type 1')\n",
    "    parser.add_argument('--group2_stage', type=int, default=1, help='Stages for type 2')\n",
    "    parser.add_argument('--group2_min_prs_time', type=int, default=10, help='Minimum processing time for type 2')\n",
    "    parser.add_argument('--group2_max_prs_time', type=int, default=300, help='Maximum processing time for type 2')\n",
    "    parser.add_argument('--prod_quantity', type=int, default=10, help='Production quantity (Unit: FOUP)')\n",
    "    parser.add_argument('--done_quantity', type=int, default=100, help='Done Production quantity (Unit: Wafer)')\n",
    "    parser.add_argument('--num_lot_type', type=int, default=2, help='Total number of lot types')\n",
    "\n",
    "    parser.add_argument('--model_type', type=str, default='concat', help='Model type = {is, rms, concat}')\n",
    "    parser.add_argument('--input_action', type=str, default='wafer', help='loadlock input action type = {wafer, type}')\n",
    "\n",
    "    return parser.parse_args([])\n",
    "\n",
    "\n",
    "def get_stage_list():\n",
    "    \"\"\"Define stage configurations.\"\"\"\n",
    "    return [\n",
    "        [1,1],\n",
    "        [1,1,1],\n",
    "    ]\n",
    "\n",
    "def setup_tester_params(args):\n",
    "    \"\"\"Initialize tester parameters.\"\"\"\n",
    "    return {\n",
    "        'use_cuda': False,\n",
    "        'cuda_device_num': CUDA_DEVICE_NUM,\n",
    "        'model_load': {\n",
    "            'enable': True,\n",
    "            'path': f'./saved_models/',\n",
    "        },\n",
    "        'multi_run_size': 1,\n",
    "        'problem_count': 100,\n",
    "        'test_batch_size': 100,\n",
    "    }\n",
    "    \n",
    "def setup_env_params(args, stage_list):\n",
    "    \"\"\"Initialize environment parameters.\"\"\"\n",
    "    return {\n",
    "        'foup_size': args.foup_size,\n",
    "        'group1_stage': stage_list[args.group1_stage],\n",
    "        'group1_min_prs_time': args.group1_min_prs_time,\n",
    "        'group1_max_prs_time': args.group1_max_prs_time,\n",
    "        'group2_stage': stage_list[args.group2_stage],\n",
    "        'group2_min_prs_time': args.group2_min_prs_time,\n",
    "        'group2_max_prs_time': args.group2_max_prs_time,\n",
    "        'prod_quantity': args.prod_quantity,\n",
    "        'done_quantity': args.done_quantity,\n",
    "        'num_lot_type': args.num_lot_type,\n",
    "    }\n",
    "    \n",
    "def setup_model_params(args, env_params):\n",
    "    \"\"\"Initialize model parameters.\"\"\"\n",
    "    return {\n",
    "        'type': args.model_type,\n",
    "        'input_action': args.input_action,\n",
    "        'purge': False,\n",
    "        'embedding_dim': 256,\n",
    "        'sqrt_embedding_dim': 256**(1/2),\n",
    "        'encoder_layer_num': 3,\n",
    "        'qkv_dim': 16,\n",
    "        'sqrt_qkv_dim': 16**(1/2),\n",
    "        'head_num': 16,\n",
    "        'logit_clipping': 10,\n",
    "        'ff_hidden_dim': 512,\n",
    "        'ms_hidden_dim': 16,\n",
    "        'ms_layer1_init': (1/2)**(1/2),\n",
    "        'ms_layer2_init': (1/16)**(1/2),\n",
    "        'eval_type': 'argmax',\n",
    "        'normalize': 'instance'\n",
    "    }\n",
    "\n",
    "\n",
    "def main(): \n",
    "    set_seed() \n",
    "    args = parse_arguments()\n",
    "    stage_list = get_stage_list()\n",
    "    \n",
    "    env_params = setup_env_params(args, stage_list)\n",
    "    model_params = setup_model_params(args, env_params)\n",
    "    tester_params = setup_tester_params(args)\n",
    "    \n",
    "    tester = Tester(env_params=env_params,\n",
    "                      model_params=model_params,\n",
    "                      tester_params=tester_params)\n",
    "    \n",
    "    result = tester.run()    \n",
    "    return result\n",
    "\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, env_params, model_params, tester_params):\n",
    "        self.env_params = env_params\n",
    "        self.model_params = model_params\n",
    "        self.tester_params = tester_params\n",
    "        self.device = torch.device('cuda' if tester_params['use_cuda'] else 'cpu')\n",
    "        self.model_params['device'] = self.device\n",
    "        self.model = self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        model = CONCATModel(**self.env_params, **self.model_params)\n",
    "        model_load = self.tester_params['model_load']\n",
    "        if model_load['enable']:\n",
    "            checkpoint_path = f\"{model_load['path']}/checkpoint_v4.pt\"\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f'[Saved Model Loaded...] -> {checkpoint_path}')\n",
    "        return model\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute the testing process.\"\"\"  \n",
    "        print(\"Running tester...\")\n",
    "        # Implement the testing process here\n",
    "        def _stack_states(states: list):\n",
    "            return State(**{field: torch.stack([getattr(state, field) for state in states])\n",
    "                            for field in State.__dataclass_fields__})\n",
    "\n",
    "        # call environments & copy for each policy\n",
    "        envs = []\n",
    "        states = []\n",
    "        for _ in range(self.tester_params['test_batch_size']):\n",
    "            env = Env(**self.env_params)\n",
    "            state = env.reset()\n",
    "            envs.append(env)\n",
    "            states.append(state)\n",
    "            \n",
    "        envs_ceil = copy.deepcopy(envs)\n",
    "        envs_floor = copy.deepcopy(envs)\n",
    "        envs_rl = copy.deepcopy(envs)\n",
    "        \n",
    "        # results storage\n",
    "        results = []\n",
    "        \n",
    "        # run css policy\n",
    "        ceil_makespans = []\n",
    "        for _, e in enumerate(envs_ceil):\n",
    "            policy_ceil = ConcurrentBackwardSequence(env, strategy='ceil')\n",
    "            while not e.done:\n",
    "                action = policy_ceil(e)\n",
    "                _ = e.step(action)\n",
    "            ceil_makespans.append(e.clock)\n",
    "    \n",
    "        floor_makespans = []\n",
    "        for _, e in enumerate(envs_floor):\n",
    "            policy_floor = ConcurrentBackwardSequence(env, strategy='floor')\n",
    "            while not e.done:\n",
    "                action = policy_floor(e)\n",
    "                _ = e.step(action)\n",
    "            floor_makespans.append(e.clock)\n",
    "\n",
    "        cbs_makespans = [ceil_makespans[i] if ceil_makespans[i] < floor_makespans[i] \n",
    "                         else floor_makespans[i] for i in range(len(ceil_makespans))]\n",
    "        \n",
    "        # prepare rl state\n",
    "        state = _stack_states(states)\n",
    "        state.batch_idx = torch.arange(state.batch_size())\n",
    "        state.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        self.model.encoding(state)\n",
    "        policy_rl = self.model\n",
    "        rl_makespans = [1e10 for _ in range(state.batch_size())]\n",
    "        \n",
    "        while not state.done.all():\n",
    "            action, prob = policy_rl(state)\n",
    "            states = []\n",
    "            for b, a in enumerate(action):\n",
    "                state = envs_rl[b].step(a.item())\n",
    "                states.append(state)\n",
    "                if envs_rl[b].done and rl_makespans[b] == 1e10:\n",
    "                    rl_makespans[b] = copy.deepcopy(envs_rl[b].clock)\n",
    "                    ## env.done之后clock也还是在继续走，所以只记录第一次到达done的makespan\n",
    "                    ## 用rl_makespans==1e10来判断是否是第一次到达done\n",
    "            state = _stack_states(states)\n",
    "            state.batch_idx = torch.arange(state.batch_size())\n",
    "            state.to(self.device)\n",
    "            \n",
    "        # collect results\n",
    "        for instance_id in range(self.tester_params['test_batch_size']):\n",
    "            type1_time = [int(i) for i in envs[instance_id].recipes[0].time[1:-1]]\n",
    "            type2_time = [int(i) for i in envs[instance_id].recipes[1].time[1:-1]]\n",
    "            results.append([instance_id, type1_time, type2_time, int(cbs_makespans[instance_id]), int(rl_makespans[instance_id])])\n",
    "        \n",
    "        # convert to DataFrame and print\n",
    "        df = pd.DataFrame(results, columns=[\"InstanceID\", \"Type 1 Process Time\", \"Type 2 Process Time\", \"CBS Makespan\", \"RL Makespan\"])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main()\n",
    "\n",
    "# print average results\n",
    "print(\"=\"*60)\n",
    "print(\"Dual-armed cluster tool with concurrent flow wafers\")\n",
    "print(f'Average makespan of CBS: {df[\"CBS Makespan\"].mean():.2f}, RL: {df[\"RL Makespan\"].mean():.2f}')\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataframe tables \n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.width', None)  \n",
    "pd.set_option('display.max_colwidth', None)  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
